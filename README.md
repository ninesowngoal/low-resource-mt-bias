# Low-Resource MT Bias

This repository contains the code and resources for my MSc dissertation project: **Bias in AI Translation Models for Low-Resource Languages**.  
The project evaluates how machine translation models handle underrepresented languages (e.g., Bemba, Swahili) and explores techniques such as LoRA adaptation and data augmentation.

## Features
- Preprocessing scripts for low-resource parallel data  
- Training configurations for baseline and adapted NLLB models  
- Evaluation scripts using BLEU, chrF, ROUGE-L, and TER  
- Example notebooks for analysis and reproducibility  

## Installation
Clone the repository and install dependencies:
```bash
git clone https://github.com/ninesowngoal/low-resource-mt-bias.git
cd low-resource-mt-bias
pip install -r requirements.txt
```
## Usage

### Run the Notebook
You can explore and reproduce the experiments directly from the Jupyter notebook:
```bash
jupyter notebook swahili_to_english_small_model.ipynb
```
### Run the Python Script
For running the translation pipeline from the command line:
```bash
python swahili_to_english_small_model.py
```
This script demonstrates how the model processes parallel data (Swahili - English) using the provided XML files.

## Data
The repository includes sample XML files:

- eng.xml — English source data
- swh.xml — Swahili target data

These files are illustrative. For larger-scale training, prepare your own dataset in a similar format.

## Results

Evaluation results (BLEU, chrF, ROUGE-L, TER) will be generated by running the notebook or script.
Example placeholder metrics (from test runs):
- BLEU: 12.9
- chrF: 43.6
- ROUGE-L (F1): 0.38
- TER: 81.6

## Citation
If you use this reporsitory, please cite:
```bash
Chishimba Chipeta, (2025). Bias in AI Translation Models for Low-Resource Languages. MSc Dissertation, Birmingham City University.
```
